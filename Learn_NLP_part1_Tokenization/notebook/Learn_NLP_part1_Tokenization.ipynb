{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **# Tokenization in NLP**"
      ],
      "metadata": {
        "id": "-LegbG0oZhSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **First we learn tokenization without any library**"
      ],
      "metadata": {
        "id": "-ww8nkKr7h4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split by white spaces\n",
        "import re\n",
        "text ='I\\'m with you for the entire life in U.K.!'\n",
        "words = re.split(r'\\W+',text)\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seo-hDCg7i39",
        "outputId": "57899213-0c10-40af-b3e6-7cfbeafe0515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'm', 'with', 'you', 'for', 'the', 'entire', 'life', 'in', 'U', 'K', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we performed basic tokenization without using any external NLP library. Tokenization is the process of breaking down a text into smaller units, usually words, so that we can analyze or process it further.\n",
        "\n",
        "Here, we used Python's `re` module (regular expressions) to split the text. The pattern `\\W+` matches any non-word characters (like spaces, punctuation, and special symbols), so the text is split wherever such characters appear."
      ],
      "metadata": {
        "id": "g9St802paXDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select words\n",
        "words = re.split(r'\\W+',text)\n",
        "print(words[:100])"
      ],
      "metadata": {
        "id": "420r9HbA7i0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d0bdaf-6f78-486f-b6f7-032b34f9d227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'm', 'with', 'you', 'for', 'the', 'entire', 'life', 'in', 'U', 'K', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we extracted all the words from a text by splitting it based on non-word characters using Python’s `re` module.\n",
        "\n",
        "* `re.split(r'\\W+', text)` tells Python to divide the text wherever a non-word character appears, such as spaces, punctuation, or symbols.\n",
        "\n",
        "* The result is a list of individual words, which makes it easier to analyze, process, or use in NLP tasks.\n",
        "\n",
        "* `print(words[:100])` simply shows the first 100 words from the list to verify our tokenization.\n",
        "\n",
        "This approach is a basic form of tokenization without using any NLP library, helping us understand how raw text can be broken down into meaningful word units."
      ],
      "metadata": {
        "id": "q8dq89Wha3gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "# split into words by white space\n",
        "words = text.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in words]\n",
        "print(stripped[:100])"
      ],
      "metadata": {
        "id": "Ex-4kmfx7ixc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b189ad98-feff-4d16-cb60-00f1dfcaa5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Im', 'with', 'you', 'for', 'the', 'entire', 'life', 'in', 'UK']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we performed tokenization with punctuation removal using basic Python techniques, without relying on any NLP library.\n",
        "\n",
        "* First, we split the text into words using the `split()` method, which separates the text at every whitespace.\n",
        "\n",
        "* Next, we created a regular expression pattern to match all punctuation characters from Python’s string.punctuation.\n",
        "\n",
        "* Using a list comprehension, we removed punctuation from each word in the list, resulting in clean words without symbols like . , ! ?.\n",
        "\n",
        "* Finally, print(stripped[:100]) displays the first 100 words after cleaning, so we can verify the output.\n",
        "\n",
        "This approach is useful for preparing text for NLP tasks, as punctuation is often unnecessary for basic word-level analysis."
      ],
      "metadata": {
        "id": "K4PgChYWbYpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# string.printable inverse of string.punctuation\n",
        "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "result = [re_print.sub('', w) for w in words]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "4ZkLOpCa7iuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b3ac11-3b02-4d15-8d34-ca009b07816a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"I'm\", 'with', 'you', 'for', 'the', 'entire', 'life', 'in', 'U.K.!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we cleaned the text to keep only printable characters using Python’s re module:\n",
        "\n",
        "* `string.printable` contains all characters that can be printed, including letters, digits, punctuation, and whitespace.\n",
        "\n",
        "* `[^%s]` in the regular expression acts as an inverse match, meaning it selects everything except printable characters.\n",
        "\n",
        "* Using a list comprehension, we removed all non-printable characters from each word in the list.\n",
        "\n",
        "* `print(result)` shows the cleaned words, ensuring the text contains only readable and standard characters, which is useful before further NLP processing.\n",
        "\n",
        "This method is helpful to filter out unwanted or invisible characters that could interfere with tokenization or text analysis."
      ],
      "metadata": {
        "id": "ysptRiDDcOxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Case\n",
        "\n",
        "# split into words by white space\n",
        "words = text.split()\n",
        "# convert to lower case\n",
        "words = [word.lower() for word in words]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "id": "KkEFzLyO7irL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd6f22f-7965-4b87-f49f-8dcf9f2cf10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"i'm\", 'with', 'you', 'for', 'the', 'entire', 'life', 'in', 'u.k.!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we **normalized the text by converting all words to lowercase**. This ensures that words like `Apple` and `apple` are treated the same, which is important for consistent text analysis in NLP.\n"
      ],
      "metadata": {
        "id": "jK43T0zvcwSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# _______________________________ Working On Spacy _________________________________\n"
      ],
      "metadata": {
        "id": "Y0zqJMoJ7in0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "voVudFJh7ie8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string = '\"I\\'m with you for the entire life in P.K.!\"'\n",
        "print(string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYeI_uZWaJRf",
        "outputId": "f90d7eec-3b1a-462c-8877-1e15a1a0fdda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"I'm with you for the entire life in P.K.!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(string)\n",
        "for token in doc:\n",
        "    print(token.text, end=' | ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N10kwHHBaSxl",
        "outputId": "d2313dbd-2c52-47af-c0ea-b257b412b499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" | I | 'm | with | you | for | the | entire | life | in | P.K. | ! | \" | "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we **used an NLP library (like SpaCy) to tokenize the text**.\n",
        "\n",
        "* `nlp(string)` processes the text and creates a **document object**.\n",
        "* The `for token in doc` loop goes through each **token (word or punctuation)** in the text.\n",
        "* `print(token.text, end=' | ')` displays each token separated by a vertical bar, so we can clearly see how the text has been split.\n",
        "\n",
        "This approach is a **library-based tokenization**, which handles words, punctuation, and special cases more accurately than manual splitting.\n"
      ],
      "metadata": {
        "id": "eFnuRChrdH8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 =nlp(u\"we're here to help! Send snail-mail, email fahad@gmail.com or visit us at https://hahad.blogspot.com/!\")\n",
        "for t in doc2:\n",
        "    print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJETl208aSu0",
        "outputId": "a2f80073-c851-40cd-e7a7-9990919ca4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we\n",
            "'re\n",
            "here\n",
            "to\n",
            "help\n",
            "!\n",
            "Send\n",
            "snail\n",
            "-\n",
            "mail\n",
            ",\n",
            "email\n",
            "fahad@gmail.com\n",
            "or\n",
            "visit\n",
            "us\n",
            "at\n",
            "https://hahad.blogspot.com/\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp(u'A 5km NYC cab ride costs $10.30')\n",
        "for t in doc3:\n",
        "    print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Krq9iWRZaSr8",
        "outputId": "f4bf3779-46b9-4b96-8eb0-a2129ad17f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "5\n",
            "km\n",
            "NYC\n",
            "cab\n",
            "ride\n",
            "costs\n",
            "$\n",
            "10.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(u\"Let's visit St. Louis in the U.S. next year.\")\n",
        "for t in doc4:\n",
        "    print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt5SmEMwaSpd",
        "outputId": "a1fb7b0b-e253-4aa1-937e-d94254db1865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let\n",
            "'s\n",
            "visit\n",
            "St.\n",
            "Louis\n",
            "in\n",
            "the\n",
            "U.S.\n",
            "next\n",
            "year\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gnUF6zTdClV",
        "outputId": "4e949688-5b23-451a-c84a-23703ecc872d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr0oSd7kdCiT",
        "outputId": "167a2d4d-0c3c-4e9b-a406-6e854d646676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "797"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc5 = nlp(u'It is better to give than to receive.')\n",
        "# Retrieve the third token:\n",
        "doc5[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhlncpBXdCf0",
        "outputId": "85f64be6-e3bb-4e51-b708-e51349a03b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "better"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve three tokens from the-middle\n",
        "doc5[2:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pnSwVuvdCdA",
        "outputId": "186edffd-7c95-4b74-cec2-187616c47903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "better to give"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve the last 4 tokens\n",
        "doc5[-4:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD5T9wKUdCaY",
        "outputId": "11ddb545-88d5-4a0e-cd4d-df071554fe71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "than to receive."
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc6 = nlp(u'My dinner was horrible.')\n",
        "doc7 = nlp(u'Your dinner was delicious.')"
      ],
      "metadata": {
        "id": "U0y7dF-DdCYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try to change 'My dinner was horrible.' to 'Your dinner was delicious.'\n",
        "doc6[3] = doc7[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "fztOQM6VeNin",
        "outputId": "3cdf2e45-7b42-4eec-e679-f585007f7bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'spacy.tokens.doc.Doc' object does not support item assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2387055611.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# try to change 'My dinner was horrible.' to 'Your dinner was delicious.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc6\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc7\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')\n",
        "\n",
        "for token in doc8:\n",
        "    print(token.text, end=' | ')\n",
        "\n",
        "print('\\n----')\n",
        "\n",
        "for ent in doc8.ents:\n",
        "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMj8Eim1eNfy",
        "outputId": "59f4de9a-6433-4868-da98-df6138173c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | \n",
            "----\n",
            "Apple - ORG - Companies, agencies, institutions, etc.\n",
            "Hong Kong - GPE - Countries, cities, states\n",
            "$6 million - MONEY - Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we **processed a text using SpaCy** to perform **tokenization and named entity recognition (NER)**:\n",
        "\n",
        "1. `doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')` processes the text and creates a SpaCy **document object**.\n",
        "2. The first `for token in doc8` loop **prints each token** (word, number, or punctuation) separated by `|`, showing how the text is split into meaningful units.\n",
        "3. The second loop `for ent in doc8.ents` identifies **named entities** in the text, such as organizations, locations, and money amounts.\n",
        "4. For each entity, `ent.text` shows the entity itself, `ent.label_` shows its type (like `ORG`, `GPE`, `MONEY`), and `spacy.explain(ent.label_)` gives a **short explanation** of the entity type.\n",
        "\n",
        "This demonstrates how SpaCy can **automatically detect important real-world entities** from text while also tokenizing it.\n"
      ],
      "metadata": {
        "id": "Duy441nndhXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc8.ents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk3rxHoxeNdF",
        "outputId": "5bce799c-358f-43d7-f40a-b5bcc7b7984d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc9 = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
        "\n",
        "for chunk in doc9.noun_chunks:\n",
        "    print(chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGqOMVYteNaT",
        "outputId": "4e3cdb95-4621-4848-a243-f47ab86c30e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autonomous cars\n",
            "insurance liability\n",
            "manufacturers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc10 = nlp(u\"Red cars do not carry higher insurance rates.\")\n",
        "\n",
        "for chunk in doc10.noun_chunks:\n",
        "    print(chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCsUZiZvfm4r",
        "outputId": "2c17237a-6a26-4539-b6df-9bc72865632e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Red cars\n",
            "higher insurance rates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc11 = nlp(u\"He was a one-eyed, one-horned, flying, purple people-eater.\")\n",
        "\n",
        "for chunk in doc11.noun_chunks:\n",
        "    print(chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxx48oLcfm2K",
        "outputId": "db68eac3-f8c1-4157-c12f-92467d37282f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He\n",
            "a one-eyed, one-horned, flying, purple people-eater\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 110})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "pU62QLdLfmzS",
        "outputId": "0641eae0-8dbe-4a5d-e856-5f47ae0432a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"16580c1b59444f3aa5a65102286958f2-0\" class=\"displacy\" width=\"1370\" height=\"357.0\" direction=\"ltr\" style=\"max-width: none; height: 357.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"160\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"160\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"270\">going</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"270\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"380\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"380\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"490\">build</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"490\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"710\">U.K.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"710\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"820\">factory</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"820\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">$</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">SYM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">6</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"267.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1260\">million.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1260\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-0\" stroke-width=\"2px\" d=\"M70,222.0 C70,112.0 260.0,112.0 260.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,224.0 L62,212.0 78,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-1\" stroke-width=\"2px\" d=\"M180,222.0 C180,167.0 255.0,167.0 255.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M180,224.0 L172,212.0 188,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-2\" stroke-width=\"2px\" d=\"M400,222.0 C400,167.0 475.0,167.0 475.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M400,224.0 L392,212.0 408,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-3\" stroke-width=\"2px\" d=\"M290,222.0 C290,112.0 480.0,112.0 480.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M480.0,224.0 L488.0,212.0 472.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-4\" stroke-width=\"2px\" d=\"M620,222.0 C620,112.0 810.0,112.0 810.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M620,224.0 L612,212.0 628,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-5\" stroke-width=\"2px\" d=\"M730,222.0 C730,167.0 805.0,167.0 805.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M730,224.0 L722,212.0 738,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-6\" stroke-width=\"2px\" d=\"M510,222.0 C510,57.0 815.0,57.0 815.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M815.0,224.0 L823.0,212.0 807.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-7\" stroke-width=\"2px\" d=\"M510,222.0 C510,2.0 930.0,2.0 930.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M930.0,224.0 L938.0,212.0 922.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-8\" stroke-width=\"2px\" d=\"M1060,222.0 C1060,112.0 1250.0,112.0 1250.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1060,224.0 L1052,212.0 1068,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-9\" stroke-width=\"2px\" d=\"M1170,222.0 C1170,167.0 1245.0,167.0 1245.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1170,224.0 L1162,212.0 1178,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-16580c1b59444f3aa5a65102286958f2-0-10\" stroke-width=\"2px\" d=\"M950,222.0 C950,57.0 1255.0,57.0 1255.0,222.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-16580c1b59444f3aa5a65102286958f2-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1255.0,224.0 L1263.0,212.0 1247.0,212.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Ph1_0REJfmwq",
        "outputId": "fea4f278-0a2a-4dd2-856e-c63eac96f2ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the last quarter\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " sold \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    nearly 20 thousand\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " iPods for a profit of \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $6 million\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'This is a sentence.')\n",
        "displacy.serve(doc, style='dep')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "Ld_nqteUfmuD",
        "outputId": "e54b5e76-e14f-4491-ee96-60381c72a6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/spacy/displacy/__init__.py:106: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
            "  warnings.warn(Warnings.W011)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"509e59f43caa4fccbb74bc6eebb4a997-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sentence.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-509e59f43caa4fccbb74bc6eebb4a997-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-509e59f43caa4fccbb74bc6eebb4a997-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-509e59f43caa4fccbb74bc6eebb4a997-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-509e59f43caa4fccbb74bc6eebb4a997-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-509e59f43caa4fccbb74bc6eebb4a997-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-509e59f43caa4fccbb74bc6eebb4a997-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>\n",
              "</figure>\n",
              "</body>\n",
              "</html></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMK4T2U4eNXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-iY0qnyeNVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Important Points About Tokenization**\n",
        "\n",
        "1. **Definition**:\n",
        "   Tokenization is the process of **breaking text into smaller units**, usually words, phrases, or sentences, called **tokens**.\n",
        "\n",
        "2. **Purpose**:\n",
        "\n",
        "   * Prepares text for **further NLP tasks** like stemming, lemmatization, or analysis.\n",
        "   * Helps in **counting words, building vocabulary, and text representation**.\n",
        "\n",
        "3. **Types of Tokenization**:\n",
        "\n",
        "   * **Word-level**: Splits text into individual words (`\"I am happy\"` → `['I', 'am', 'happy']`).\n",
        "   * **Sentence-level**: Splits text into sentences.\n",
        "   * **Subword or character-level**: Used in advanced NLP, like BERT or GPT models.\n",
        "\n",
        "4. **Methods**:\n",
        "\n",
        "   * **Manual/Regex-based**: Using Python methods (`split()`) or regular expressions (`re.split()`).\n",
        "   * **Library-based**: Using NLP libraries like **NLTK** or **SpaCy**, which handle punctuation, contractions, and special cases better.\n",
        "\n",
        "5. **Considerations**:\n",
        "\n",
        "   * **Case normalization**: Convert text to lowercase to treat `Apple` and `apple` the same.\n",
        "   * **Punctuation**: Decide whether to remove punctuation depending on the task.\n",
        "   * **Special characters**: Remove non-printable or unwanted characters for clean tokens.\n",
        "   * **Handling contractions**: Some libraries handle `\"I'm\"` → `['I', \"'m\"]` correctly.\n",
        "   * **Language-specific rules**: Tokenization may differ for languages with different writing systems.\n",
        "\n",
        "6. **Output**:\n",
        "   The result of tokenization is typically a **list of tokens** that can be used in further NLP tasks like text analysis, vectorization, or named entity recognition.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "L0-oeI7yeJrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Limitations of Tokenization**\n",
        "\n",
        "1. **Ambiguity in Words**:\n",
        "   Words with multiple meanings or abbreviations can be misinterpreted. For example, `US` could mean \"United States\" or \"us\" depending on context.\n",
        "\n",
        "2. **Handling Contractions**:\n",
        "   Simple tokenization may fail to split contractions correctly, e.g., `\"I'm\"` might be treated as one token instead of `[\"I\", \"'m\"]`.\n",
        "\n",
        "3. **Punctuation Issues**:\n",
        "   Deciding whether to keep or remove punctuation can affect meaning, especially in questions, exclamations, or URLs.\n",
        "\n",
        "4. **Complex Languages**:\n",
        "   Languages without clear word boundaries (like Chinese, Japanese) require **special tokenizers**, as simple whitespace splitting fails.\n",
        "\n",
        "5. **Numbers, Symbols, and Emojis**:\n",
        "   Tokenizers may not always handle numbers, currency, hashtags, or emojis correctly, which can affect analysis.\n",
        "\n",
        "6. **Context Ignorance**:\n",
        "   Basic tokenization doesn’t understand **semantic meaning**, so “Apple” (company) vs “apple” (fruit) are treated the same without additional NLP processing.\n",
        "\n",
        "7. **Inconsistent Outputs Across Tools**:\n",
        "   Different libraries (NLTK, SpaCy, etc.) may tokenize the **same text differently**, making results inconsistent if not standardized.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B3i8_1joejwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Pros of Tokenization**\n",
        "\n",
        "1. **Simplifies Text Processing** – Breaks text into manageable units for analysis.\n",
        "2. **Foundation for NLP Tasks** – Essential for stemming, lemmatization, vectorization, and NER.\n",
        "3. **Flexible Methods** – Can be done manually (regex) or with libraries (NLTK, SpaCy).\n",
        "4. **Supports Multi-Level Analysis** – Works at word, sentence, or subword level.\n",
        "5. **Improves Consistency** – When combined with case normalization, it helps treat similar words uniformly.\n",
        "\n",
        "---\n",
        "\n",
        "### **Cons of Tokenization**\n",
        "\n",
        "1. **Ambiguity Handling** – Can misinterpret abbreviations, contractions, or special cases.\n",
        "2. **Language Limitations** – Whitespace-based tokenization fails for languages like Chinese or Japanese.\n",
        "3. **Punctuation and Symbols** – Requires careful handling; may lose important information.\n",
        "4. **Context Ignorance** – Doesn’t understand meaning; “Apple” the company vs “apple” the fruit are treated the same.\n",
        "5. **Inconsistent Outputs** – Different tools may tokenize the same text differently.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cuLSmqt7eqJc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nXSP7BtneOyq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}